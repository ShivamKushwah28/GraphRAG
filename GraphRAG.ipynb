{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install langchain transformers faiss-cpu pandas sentence-transformers networkx bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmtru1F2_nZ4",
        "outputId": "25d66d80-e52b-448a-e47e-253be9f9d581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.25)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login #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"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCJejSOjBDOS",
        "outputId": "eadc15f4-36eb-4b49-8036-72543ac940d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) \n",
            "Token is valid (permission: write).\n",
            "The token `SAMPLE` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `SAMPLE`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import json\n",
        "\n",
        "# 1. **Preprocessing Module**: Filter data and combine defect, consequence, and corrective action summaries\n",
        "def preprocess_data(file_path, makes):\n",
        "    \"\"\"\n",
        "    Preprocesses the dataset by filtering for specific makes and creating a combined summary.\n",
        "\n",
        "    Args:\n",
        "    - file_path (str): Path to the dataset file (CSV).\n",
        "    - makes (list): List of makes to filter on (e.g., ['ford', 'toyota']).\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: Processed dataset with columns 'MAKETXT', 'MODELTXT', 'YEARTXT', and 'combined_summary'.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Filter for the specified makes (case insensitive)\n",
        "    df_filtered = df[df['MAKETXT'].str.lower().isin([make.lower() for make in makes])]\n",
        "\n",
        "    # Combine the defect, consequence, and corrective action summaries\n",
        "    df_filtered['combined_summary'] = df_filtered[['DESC_DEFECT', 'CONSEQUENCE_DEFECT', 'CORRECTIVE_ACTION']].apply(\n",
        "        lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
        "\n",
        "    return df_filtered[['MAKETXT', 'MODELTXT', 'YEARTXT', 'combined_summary']]\n",
        "\n",
        "# 2. **Embedding Module**: Generate embeddings for the combined summaries\n",
        "def generate_embeddings(documents):\n",
        "    \"\"\"\n",
        "    Generates embeddings for a list of documents using a sentence transformer model.\n",
        "\n",
        "    Args:\n",
        "    - documents (list of str): List of text documents (combined defect, consequence, and corrective action).\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: Array of document embeddings.\n",
        "    \"\"\"\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    embeddings = model.encode(documents, convert_to_numpy=True)\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "# 3. **Graph Construction Module**: Construct a similarity graph using cosine similarity\n",
        "def construct_similarity_graph(embeddings, similarity_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Constructs a graph where nodes represent documents and edges represent similarities.\n",
        "\n",
        "    Args:\n",
        "    - embeddings (np.ndarray): Document embeddings.\n",
        "    - similarity_threshold (float): Threshold for creating edges based on cosine similarity.\n",
        "\n",
        "    Returns:\n",
        "    - nx.Graph: A NetworkX graph where nodes are documents and edges represent similarity.\n",
        "    \"\"\"\n",
        "    # Compute cosine similarity matrix\n",
        "    cosine_sim = cosine_similarity(embeddings)\n",
        "\n",
        "    # Create a graph\n",
        "    graph = nx.Graph()\n",
        "\n",
        "    # Add nodes (documents)\n",
        "    for idx in range(len(embeddings)):\n",
        "        graph.add_node(idx, text=f\"Document {idx}\")\n",
        "\n",
        "    # Add edges based on similarity threshold\n",
        "    for i in range(len(embeddings)):\n",
        "        for j in range(i + 1, len(embeddings)):\n",
        "            if cosine_sim[i, j] > similarity_threshold:\n",
        "                graph.add_edge(i, j, weight=cosine_sim[i, j])\n",
        "\n",
        "    return graph\n",
        "\n",
        "# 4. **Retrieval Module**: Retrieve the top-k most relevant documents for a query\n",
        "def retrieve_relevant_documents(query, embeddings, top_k=5):\n",
        "    \"\"\"\n",
        "    Retrieves the top-k most relevant documents for a given query.\n",
        "\n",
        "    Args:\n",
        "    - query (str): The query text.\n",
        "    - embeddings (np.ndarray): The embeddings of the dataset documents.\n",
        "    - top_k (int): Number of documents to retrieve.\n",
        "\n",
        "    Returns:\n",
        "    - list: List of retrieved documents.\n",
        "    \"\"\"\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
        "    cosine_sim = cosine_similarity(query_embedding, embeddings)\n",
        "\n",
        "    # Get indices of the top-k most similar documents\n",
        "    top_indices = np.argsort(cosine_sim[0])[-top_k:][::-1]\n",
        "\n",
        "    return top_indices\n",
        "\n",
        "# 5. **Summarization Module**: Generate a summary of the retrieved documents\n",
        "def generate_summary(retrieved_documents):\n",
        "    \"\"\"\n",
        "    Generates a summary for the retrieved documents.\n",
        "\n",
        "    Args:\n",
        "    - retrieved_documents (list of str): List of text documents.\n",
        "\n",
        "    Returns:\n",
        "    - str: The generated summary.\n",
        "    \"\"\"\n",
        "    # Load pre-trained T5 model for summarization\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
        "\n",
        "    # Combine retrieved documents into one text\n",
        "    input_text = \" \".join(retrieved_documents)\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Generate summary\n",
        "    summary_ids = model.generate(inputs['input_ids'], max_length=150, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
        "\n",
        "    # Decode the summary\n",
        "    final_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return final_summary\n",
        "\n",
        "# 6. **Main Pipeline**: Combine all modules into a complete workflow\n",
        "def main(input_json, dataset_file, makes=['ford', 'toyota']):\n",
        "    # Step 1: Load and preprocess data\n",
        "    dataset = preprocess_data(dataset_file, makes)\n",
        "\n",
        "    # Step 2: Generate document embeddings\n",
        "    embeddings = generate_embeddings(dataset['combined_summary'].tolist())\n",
        "\n",
        "    # Step 3: Construct similarity graph (optional for GraphRAG enhancement)\n",
        "    graph = construct_similarity_graph(embeddings)\n",
        "\n",
        "    # Step 4: Retrieve relevant documents\n",
        "    query = f\"{input_json['issue']} for {input_json['make']} {input_json['model']} {input_json['year']}\"\n",
        "    top_indices = retrieve_relevant_documents(query, embeddings)\n",
        "\n",
        "    # Get the retrieved documents\n",
        "    retrieved_docs = [dataset.iloc[idx]['combined_summary'] for idx in top_indices]\n",
        "\n",
        "    # Step 5: Generate the summary\n",
        "    summary = generate_summary(retrieved_docs)\n",
        "\n",
        "    # Return output\n",
        "    output = {\n",
        "        'retrieved_documents': retrieved_docs,\n",
        "        'summary': summary\n",
        "    }\n",
        "\n",
        "    return json.dumps(output, indent=2)\n",
        "\n",
        "# Example Input\n",
        "input_json = {\n",
        "    'make': 'ford',\n",
        "    'model': 'escape',\n",
        "    'year': '2001',\n",
        "    'issue': 'stuck throttle risk'\n",
        "}\n",
        "\n",
        "# Run the pipeline\n",
        "dataset_file = '/content/FLAT_RCL.csv'\n",
        "result = main(input_json, dataset_file)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZjfHAXpIE-5",
        "outputId": "af601148-555b-48dc-d629-5cb1cd03189d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-d1224c187a1d>:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_filtered['combined_summary'] = df_filtered[['DESC_DEFECT', 'CONSEQUENCE_DEFECT', 'CORRECTIVE_ACTION']].apply(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"retrieved_documents\": [\n",
            "    \"Ford Motor Company is recalling certain model year 2001 through 2004 Escape vehicles equipped with 3.0L V6 engines and speed control manufactured from October 22, 1999, through January 23, 2004.  Inadequate clearance between the engine cover and the speed control cable connector could result in a stuck throttle when the accelerator pedal is fully or almost-fully depressed.  This risk exists regardless of whether or not speed control (cruise control) is used. A stuck throttle may result in very high vehicle speeds and make it difficult to stop or slow the vehicle, which could cause a crash, serious injury or death.  Ford will notify owners, and dealers will repair the vehicles by increasing the engine cover clearance, free of charge.  The safety recall began August 3, 2012.  Remedy parts are expected to be available in mid-August 2012.  Until then dealers will disconnect the speed control cable as an interim remedy, if parts are not available at the time of an owner's service appointment.  Owners may contact Ford at 1-866-436-7332. \",\n",
            "    \"Ford Motor Company is recalling certain model year 2001 through 2004 Escape vehicles equipped with 3.0L V6 engines and speed control manufactured from October 22, 1999, through January 23, 2004.  Inadequate clearance between the engine cover and the speed control cable connector could result in a stuck throttle when the accelerator pedal is fully or almost-fully depressed.  This risk exists regardless of whether or not speed control (cruise control) is used. A stuck throttle may result in very high vehicle speeds and make it difficult to stop or slow the vehicle, which could cause a crash, serious injury or death.  Ford will notify owners, and dealers will repair the vehicles by increasing the engine cover clearance, free of charge.  The safety recall began August 3, 2012.  Remedy parts are expected to be available in mid-August 2012.  Until then dealers will disconnect the speed control cable as an interim remedy, if parts are not available at the time of an owner's service appointment.  Owners may contact Ford at 1-866-436-7332. \",\n",
            "    \"Ford Motor Company is recalling certain model year 2001 through 2004 Escape vehicles equipped with 3.0L V6 engines and speed control manufactured from October 22, 1999, through January 23, 2004.  Inadequate clearance between the engine cover and the speed control cable connector could result in a stuck throttle when the accelerator pedal is fully or almost-fully depressed.  This risk exists regardless of whether or not speed control (cruise control) is used. A stuck throttle may result in very high vehicle speeds and make it difficult to stop or slow the vehicle, which could cause a crash, serious injury or death.  Ford will notify owners, and dealers will repair the vehicles by increasing the engine cover clearance, free of charge.  The safety recall began August 3, 2012.  Remedy parts are expected to be available in mid-August 2012.  Until then dealers will disconnect the speed control cable as an interim remedy, if parts are not available at the time of an owner's service appointment.  Owners may contact Ford at 1-866-436-7332. \",\n",
            "    \"Ford Motor Company is recalling certain model year 2001 through 2004 Escape vehicles equipped with 3.0L V6 engines and speed control manufactured from October 22, 1999, through January 23, 2004.  Inadequate clearance between the engine cover and the speed control cable connector could result in a stuck throttle when the accelerator pedal is fully or almost-fully depressed.  This risk exists regardless of whether or not speed control (cruise control) is used. A stuck throttle may result in very high vehicle speeds and make it difficult to stop or slow the vehicle, which could cause a crash, serious injury or death.  Ford will notify owners, and dealers will repair the vehicles by increasing the engine cover clearance, free of charge.  The safety recall began August 3, 2012.  Remedy parts are expected to be available in mid-August 2012.  Until then dealers will disconnect the speed control cable as an interim remedy, if parts are not available at the time of an owner's service appointment.  Owners may contact Ford at 1-866-436-7332. \",\n",
            "    \"Ford Motor Company is recalling certain model year 2001 through 2004 Escape vehicles equipped with 3.0L V6 engines and speed control manufactured from October 22, 1999, through January 23, 2004.  Inadequate clearance between the engine cover and the speed control cable connector could result in a stuck throttle when the accelerator pedal is fully or almost-fully depressed.  This risk exists regardless of whether or not speed control (cruise control) is used. A stuck throttle may result in very high vehicle speeds and make it difficult to stop or slow the vehicle, which could cause a crash, serious injury or death.  Ford will notify owners, and dealers will repair the vehicles by increasing the engine cover clearance, free of charge.  The safety recall began August 3, 2012.  Remedy parts are expected to be available in mid-August 2012.  Until then dealers will disconnect the speed control cable as an interim remedy, if parts are not available at the time of an owner's service appointment.  Owners may contact Ford at 1-866-436-7332. \"\n",
            "  ],\n",
            "  \"summary\": \"Ford at 1-866-436-7332. Ford Motor Company is recalling certain model year 2001 through 2004 Escape vehicles equipped with 3.0L V6 engines and speed control manufactured from October 22, 1999, through January 23, 2004. Inadequate clearance between the engine cover and the speed control cable connector could result in a stuck throttle when the accelerator pedal is fully or almost-fully depressed. This risk exists regardless of whether or not speed control (cruise control) is used. A stuck throttle may result in very high vehicle speeds and make it\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Answers**\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Why do you think the basic RAG approach fails in such situations?**\n",
        "\n",
        "The basic RAG approach fails in holistic queries such as \"most frequent recall\" or \"top 5 recalls\" due to the following reasons:\n",
        "\n",
        "1. **Local View Limitation**:\n",
        "   - RAG retrieves and processes only a small subset of documents relevant to the query. While efficient for specific questions, it lacks a global perspective required for broader insights.\n",
        "\n",
        "2. **Lack of Aggregation**:\n",
        "   - RAG does not perform aggregate computations like counting, ranking, or summarizing patterns across the entire dataset.\n",
        "\n",
        "3. **Retrieval Over Analysis**:\n",
        "   - RAG emphasizes retrieving relevant documents and generating text but lacks the tools for data analysis to identify trends or patterns.\n",
        "\n",
        "4. **Context Fragmentation**:\n",
        "   - Documents are processed individually, which leads to a fragmented understanding of trends across the dataset.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. What are some methods that can be employed to improve RAG for holistic questions?**\n",
        "\n",
        "To improve RAG for holistic questions, the following methods can be applied:\n",
        "\n",
        "1. **Clustering-Based Retrieval**:\n",
        "   - Group documents with similar defect summaries using clustering techniques (e.g., K-means, hierarchical clustering).\n",
        "   - Use cluster-level information for broader and more comprehensive insights.\n",
        "\n",
        "2. **Graph-Based Query Expansion**:\n",
        "   - Construct a knowledge graph where nodes represent issues or recalls and edges represent relationships (e.g., frequency, co-occurrence).\n",
        "   - Traverse the graph to identify clusters or frequent patterns.\n",
        "\n",
        "3. **Hybrid Models**:\n",
        "   - Combine RAG with analytical or statistical models to extract structured outputs like rankings, trends, or aggregate statistics alongside textual summaries.\n",
        "\n",
        "4. **Precomputed Metrics**:\n",
        "   - Precompute insights such as the most frequent recalls, top models, or yearly trends during preprocessing to speed up holistic queries.\n",
        "\n",
        "5. **Semantic Aggregation**:\n",
        "   - Use embedding-based models to create semantic clusters and compute aggregate insights (e.g., using cosine similarity or vector averaging).\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Can you think of some preprocessing that can be done on the dataset to aid in the existing RAG pipeline?**\n",
        "\n",
        "Preprocessing steps to improve RAG for holistic questions include:\n",
        "\n",
        "1. **Clustering**:\n",
        "   - Cluster defect summaries based on textual embeddings to group similar recalls.\n",
        "   - Save cluster IDs for efficient retrieval during holistic queries.\n",
        "\n",
        "2. **Graph Enhancements**:\n",
        "   - Add edge weights to the graph to represent recall frequency or severity.\n",
        "   - Introduce relationships such as similarity scores, co-occurrence, or corrective action overlap.\n",
        "\n",
        "3. **Index-Based Aggregation**:\n",
        "   - Precompute frequent issues, top recalls, and yearly trends. Store these insights in a separate index for quick access.\n",
        "\n",
        "4. **Normalization and Cleanup**:\n",
        "   - Normalize text data by correcting typos, removing duplicates, and unifying formats (e.g., for make, model, year).\n",
        "\n",
        "5. **Ranking and Metadata**:\n",
        "   - Add columns to the dataset for recall rankings by frequency, severity, or year. Use this data for better retrieval.\n",
        "\n",
        "6. **Embedding Preprocessing**:\n",
        "   - Use models like SentenceTransformers to compute and store embeddings for all defect summaries.\n",
        "   - Utilize these embeddings to identify patterns or aggregate trends.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "UWCeLREAJinU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pLVQuG8RJg8i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}